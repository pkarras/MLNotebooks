{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning hand-in 4\n",
    "This handin is about implementing and using representative-based clustering algorithms. If you have made your way through the programming exercises for weeks 11 and 12 you are almost done with the first two parts of the handin. \n",
    "\n",
    "The handin is mandatory, and should be done in groups of 2-3 students. Each group\n",
    "must prepare a report in PDF format as outlined below. Please submit all your\n",
    "Python files in a zip file, and your PDF report outside the zip file, to\n",
    "Blackboard no later than **Monday, December 10 at 8:00 AM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing the Algorithms\n",
    "In this section you must implement Lloyds algorithm andthe Gaussian Mixture Expectation\n",
    "Maximization algorithm. Please refer to p. 349 and p. 335 of the textbook [ZM] or see the exercises from <a href=\"https://github.com/AlexanderMath/MLNotebooks/blob/master/AU%20ML%20course/Week%2011.ipynb\" target=\"new\">week 11</a>/12. If you already did this during class copy paste them in below. Like in class, you should test your algorithms on the Iris data set:\n",
    "\n",
    "<!-- \n",
    "For the EM algorithm, you may use the `pdf` function below to compute the probability\n",
    "densities in the Gaussian Mixture model.\n",
    "\n",
    "You should use the Python code displayed previously to load and display Iris data\n",
    "and apply PCA to reduce it from four dimensions to two.\n",
    "\n",
    "Use the 2d Iris data to validate your algorithms (compare the results you get\n",
    "with the results in the textbook on the same data), and run your algorithms on\n",
    "the 4d data and compare.\n",
    "\n",
    "You should read the deliverables section at the very end of the notebook before getting into detail with the code below.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the Iris data set\n",
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris['data'][:,0:2] # reduce to 2d so you can plot if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Lloyd's Algorithm\n",
    "You are allowed to structure your code however you want. The template below is identical to the one given in class at <a href=\"https://github.com/AlexanderMath/MLNotebooks/blob/master/AU%20ML%20course/Week%2011.ipynb\" target=\"new\">week 11</a>. We recommend you use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lloyds_algorithm(X, k, T):\n",
    "    \"\"\" Clusters the data of X into k clusters using T iterations of Lloyd's algorithm. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data matrix of shape (n, d)\n",
    "        k : Number of clusters.\n",
    "        T : Maximum number of iterations to run Lloyd's algorithm. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        clustering: A vector of shape (n, ) where the i'th entry holds the cluster of X[i].\n",
    "        centroids:  The centroids/average points of each cluster. \n",
    "        cost:       The cost of the clustering \n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Initialize clusters random. \n",
    "    clustering = np.random.randint(0, k, (n, )) \n",
    "    centroids  = np.zeros((k, d))\n",
    "    \n",
    "    # Used to stop if cost isn't improving (decreasing)\n",
    "    cost = 0\n",
    "    oldcost = 0\n",
    "    \n",
    "    # Column names\n",
    "    print(\"Iterations\\tCost\")\n",
    "    \n",
    "    for i in range(T):\n",
    "        # Update centroid\n",
    "        # YOUR CODE HERE\n",
    "        # END CODE\n",
    "\n",
    "        \n",
    "        # Update clustering \n",
    "        # YOUR CODE HERE\n",
    "        # END CODE\n",
    "        \n",
    "        # Compute and print cost\n",
    "        cost = 0\n",
    "        for j in range(n):\n",
    "            cost += np.linalg.norm(X[j] - centroids[clustering[j]])**2    \n",
    "        print(i+1, \"\\t\\t\", cost)\n",
    "        \n",
    "        # Stop if cost didn't improve more than epislon (decrease)\n",
    "        if np.isclose(cost, oldcost): break #TODO\n",
    "        oldcost = cost\n",
    "        \n",
    "    return clustering, centroids, cost\n",
    "\n",
    "clustering, centroids, cost = lloyds_algorithm(X, 3, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Expectation Maximization Algorithm\n",
    "Again, you are allowed to structure your code however you want. The template below (and the helper function `compute_probs_cx`) is identical to the one given in class at <a href=\"https://github.com/AlexanderMath/MLNotebooks/blob/master/AU%20ML%20course/Week-12.ipynb\" target=\"new\">week 12</a>. We recommend you use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def compute_probs_cx(points, means, covs, probs_c):\n",
    "    '''\n",
    "    Input\n",
    "      - points: (n times d) array containing the dataset\n",
    "      - means:  (k times d) array containing the k means\n",
    "      - covs:   (k times d times d) array such that cov[j,:,:] is the covariance matrix of the j-th Gaussian.\n",
    "      - priors: (k) array containing priors\n",
    "    Output\n",
    "      - probs:  (k times n) array such that the entry (i,j) represents Pr(C_i|x_j)\n",
    "    '''\n",
    "    # Convert to numpy arrays.\n",
    "    points, means, covs, probs_c = np.asarray(points), np.asarray(means), np.asarray(covs), np.asarray(probs_c)\n",
    "    \n",
    "    # Get sizes\n",
    "    n, d = points.shape\n",
    "    k = means.shape[0]\n",
    "    \n",
    "    # Compute probabilities\n",
    "    # This will be a (k, n) matrix where the (i,j)'th entry is Pr(C_i)*Pr(x_j|C_i).\n",
    "    probs_cx = np.zeros((k, n))\n",
    "    for i in range(k):\n",
    "        try:\n",
    "            probs_cx[i] = probs_c[i] * multivariate_normal.pdf(mean=means[i], cov=covs[i], x=points)\n",
    "        except Exception as e:\n",
    "            print(\"Cov matrix got singular: \", e)\n",
    "    \n",
    "    # The sum of the j'th column of this matrix is P(x_j); why?\n",
    "    probs_x = np.sum(probs_cx, axis=0, keepdims=True) \n",
    "    assert probs_x.shape == (1, n)\n",
    "    \n",
    "    # Divide the j'th column by P(x_j). The the (i,j)'th then \n",
    "    # becomes Pr(C_i)*Pr(x_j)|C_i)/Pr(x_j) = Pr(C_i|x_j)\n",
    "    probs_cx = probs_cx / probs_x\n",
    "    \n",
    "    return probs_cx, probs_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_algorithm(X, k, T, epsilon = 0.001, means=None):\n",
    "    \"\"\" Clusters the data X into k clusters using the Expectation Maximization algorithm. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data matrix of shape (n, d)\n",
    "        k : Number of clusters.\n",
    "        T : Maximum number of iterations\n",
    "        epsilon :  Stopping criteria for the EM algorithm. Stops if the means of\n",
    "                   two consequtive iterations are less than epsilon.\n",
    "        means : (k times d) array containing the k initial means (optional)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        means:     (k, d) array containing the k means\n",
    "        covs:      (k, d, d) array such that cov[j,:,:] is the covariance matrix of \n",
    "                   the Gaussian of the j-th cluster\n",
    "        probs_c:   (k, ) containing the probability Pr[C_i] for i=0,...,k. \n",
    "        llh:       The log-likelihood of the clustering (this is the objective we want to maximize)\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Initialize and validate mean\n",
    "    if means is None: \n",
    "        means = np.random.rand(k, d)\n",
    "\n",
    "    # Initialize cov, prior\n",
    "    probs_x  = np.zeros(n) \n",
    "    probs_cx = np.zeros((k, n)) \n",
    "    probs_c  = np.zeros(k) + np.random.rand(k)\n",
    "    \n",
    "    covs = np.zeros((k, d, d))\n",
    "    for i in range(k): covs[i] = np.identity(d)\n",
    "    probs_c = np.ones(k) / k\n",
    "    \n",
    "    # Column names\n",
    "    print(\"Iterations\\tLLH\")\n",
    "    \n",
    "    close = False\n",
    "    old_means = np.zeros_like(means)\n",
    "    iterations = 0\n",
    "    while not(close) and iterations < T:\n",
    "        old_means[:] = means \n",
    "\n",
    "        # Expectation step\n",
    "        probs_cx, probs_x = compute_probs_cx(X, means, covs, probs_c)\n",
    "        assert probs_cx.shape == (k, n)\n",
    "        \n",
    "        # Maximization step\n",
    "        # YOUR CODE HERE\n",
    "        # END CODE\n",
    "        \n",
    "        # Compute per-sample average log likelihood (llh) of this iteration     \n",
    "        llh = 1/n*np.sum(np.log(probs_x))\n",
    "        print(iterations+1, \"\\t\\t\", llh)\n",
    "\n",
    "        # Stop condition\n",
    "        dist = np.sqrt(((means - old_means) ** 2).sum(axis=1))\n",
    "        close = np.all(dist < epsilon)\n",
    "        iterations += 1\n",
    "        \n",
    "    # Validate output\n",
    "    assert means.shape == (k, d)\n",
    "    assert covs.shape == (k, d, d)\n",
    "    assert probs_c.shape == (k,)\n",
    "    \n",
    "    return means, covs, probs_c, llh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one wants a hard clustering where each point belongs strictly to one cluster, one could assign each point to the cluster that maximize $\\Pr(C_i \\mid x)$ for $i=1,..,k$. You should implement the following method that does this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_em_cluster(means, covs, probs_c, data):\n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Combine Lloyd's algorithm with the Expectation Maximization algorithm \n",
    "In order to determine an initial set of cluster centers for the EM algorithm, one can utilize the best centroids determined by Lloyd's algorithm. For this, run Lloyd's algorithm several times, and choose the best one. Then run the EM algorithm with the initial means as the centroids obtained in the previous step.\n",
    "\n",
    "Test your implementation on the Iris dataset. Compare the results with Lloyd's Algorithm and the Expectation Maximization algorithm (without Lloyd's initialization). You could compare the clusterings by visualizing them like we did in class, or by using the evaluation measure from the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the Iris data set\n",
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris['data'][:,0:2] # reduce to 2d so you can plot if you want\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Report section 1: </b>\n",
    "- Explain how you compared the results of Lloyd's algoithm and the Expectation Maximization algorithm. \n",
    "- Which algorithm performed best? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating clusterings\n",
    "\n",
    "Implement the F1 score (build the contingency table p. 426, measure pp. 427-428)\n",
    "and the silhouette coefficient (pp. 444-445), and compare the quality of\n",
    "several runs of your algorithms with different values for *k*. \n",
    "\n",
    "You will need labels to compute the F1 score. These are imported below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris data set\n",
    "import sklearn.datasets\n",
    "X, y = sklearn.datasets.load_iris(True)\n",
    "X = X[:,0:2] # reduce to 2d so you can plot if you want\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Silhouette Coefficient\n",
    "Implement the Silhouette coefficient as explained during the lectures and class week 11 and 12. Explanations can be found both on slides and in the book. You can use the following template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def silhouette(data, clustering): \n",
    "    n, d = data.shape\n",
    "    k = np.unique(clustering)[-1]+1\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    silh = None\n",
    "    # END CODE\n",
    "\n",
    "    return silh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run Lloyd's algorithm and the Expectation Maximization algorithm with different $k$ and compute the silhouette coefficient for each. You could use the following template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(2, 10):\n",
    "    #... = em_algorithm(X, k, 50)\n",
    "    # em_sc = silhouette(...)\n",
    "    print(k, em_sc)\n",
    "    \n",
    "    #lloyds_algorithm(X, k, 50)\n",
    "    # lloyd_sc = silhouette(...)\n",
    "    print(k, lloyd_sc)\n",
    "    \n",
    "    # (Optional) try the lloyd's initialized EM algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Report 2.1: </b>\n",
    "- Include a tabel that holds the silhouette coefficient for the above experiment.\n",
    "- Which value of $k$ gave the best silhouette coefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 F1 Score\n",
    "Implement a function that compute the F1 score. There are explanations in the slides and in the book. You can use the following template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(predicted, labels):\n",
    "    n, = predicted.shape\n",
    "    assert labels.shape == (n,)\n",
    "    r = np.max(predicted) + 1\n",
    "    k = np.max(labels) + 1\n",
    "\n",
    "    # Implement the F1 score here\n",
    "    # YOUR CODE HERE\n",
    "    contingency = None\n",
    "    F_individual = None\n",
    "    F_overall = None\n",
    "    # END CODE\n",
    "\n",
    "    assert contingency.shape == (r, k)\n",
    "    return F_individual, F_overall, contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a similar experiment as the one for the silhouette coefficient. In other words: try to run Lloyd's algorithm and the Expectation Maximization algorithm with different k and compute the F1 score for each. You could use the following template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(2, 10):\n",
    "    #... = em_algorithm(X, k, 50)\n",
    "    # em_sc = f1(...)\n",
    "    print(k, em_sc)\n",
    "    \n",
    "    #lloyds_algorithm(X, k, 50)\n",
    "    # lloyd_sc = f1(...)\n",
    "    print(k, lloyd_sc)\n",
    "    \n",
    "    # (optional) Try the lloyd's initialized EM algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Report section 2.2: </b>\n",
    "- Include a tabel that holds the silhouette coefficient for the above experiment.\n",
    "- Which value of $k$ gave the best silhouette coefficient?\n",
    "\n",
    "<b>Report section 2.3: </b>Are there any differences between the two quality measures (F1 and Silhouette Coefficient)?\n",
    "\n",
    "HINT: External/internal measures supervised/unsupervised.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compressing images\n",
    "In class week 11 you saw how to do image compression using clustering. In this section you will revisit image compression. Find a picture you like. You could Google your favourite animal, car, disney cartoon character or take a photo of your study group. You should now use Lloyd's algorithm, EM or a combination to compress your image. \n",
    "\n",
    "The size of images can be measured in bytes. The following code downloads an image, displays it and prints the size of the images in bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def download_image(url):\n",
    "    filename = url[url.rindex('/')+1:]\n",
    "    try:\n",
    "        with open(filename, 'rb') as fp:\n",
    "            return imageio.imread(fp) / 255\n",
    "    except FileNotFoundError:\n",
    "        import urllib.request\n",
    "        with open(filename, 'w+b') as fp, urllib.request.urlopen(url) as r:\n",
    "            fp.write(r.read())\n",
    "            return imageio.imread(fp) / 255\n",
    " \n",
    "img_facade = download_image('https://uploads.toptal.io/blog/image/443/toptal-blog-image-1407508081138.png')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "ax.imshow(img_facade)\n",
    "plt.show()\n",
    "\n",
    "size = os.stat('toptal-blog-image-1407508081138.png').st_size\n",
    "\n",
    "print(\"The image consumes a total of %i bytes. \\n\"%size)\n",
    "print(\"You should compress your image as much as possible! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code downloads an image of the Nygaard building and attempts to compress it with Lloyd's algorithm as we did in class. It then saves the compressed version, prints the size of the original and the compressed version; it finally prints the compression ratio.  \n",
    "\n",
    "Modify the code such that it compresses your image; <font color=\"red\">you should get a compression ratio of at least 1.5</font>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def compress_kmeans(im, k, T, name):\n",
    "    height, width, depth = im.shape\n",
    "    data = im.reshape((height * width, depth))\n",
    "    clustering, centroids, score = lloyds_algorithm(data, k, 5)\n",
    "    \n",
    "    # make each entry of data to the value of it's cluster\n",
    "    data_compressed = data\n",
    "    \n",
    "    for i in range(k): data_compressed[clustering == i] = centroids[i] \n",
    "    \n",
    "    im_compressed = data_compressed.reshape((height, width, depth))\n",
    "    \n",
    "    # The following code should not be changed. \n",
    "    fig = plt.figure(frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    plt.imshow(im_compressed)\n",
    "    plt.savefig(\"compressed.jpg\")\n",
    "    plt.show()\n",
    "    \n",
    "    original_size   = os.stat(name).st_size\n",
    "    compressed_size = os.stat('compressed.jpg').st_size\n",
    "    print(\"Original Size: \\t\\t\", original_size)\n",
    "    print(\"Compressed Size: \\t\", compressed_size)\n",
    "    print(\"Compression Ratio: \\t\", round(original_size/compressed_size, 5))\n",
    "\n",
    "def compress_facade(k=4, T=100):\n",
    "    img_facade = download_image('https://users-cs.au.dk/rav/ml/handins/h4/nygaard_facade.jpg')\n",
    "    compress_kmeans(img_facade, k, T, 'nygaard_facade.jpg')\n",
    "    \n",
    "compress_facade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Report section 3: </b>\n",
    "- Include the original and the compressed image. You should state the original and compressed sizes and the compression ratio. \n",
    "- Briefly in 5 lines write what you did to compress you image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sampling from MNIST\n",
    "The code below runs `sklearn`'s implementation of EM on the MNIST dataset. It then prints the clusters which should look like digits. Because the clusters are represented as gaussians we can sample from them. There is a function called `sample_digits` below which can sample digits. \n",
    "\n",
    "Try run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.mixture import GaussianMixture as EM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\")\n",
    "\n",
    "X = mnist.train.images\n",
    "y = mnist.train.labels\n",
    "\n",
    "# One cluster for each digit\n",
    "k = 10\n",
    "\n",
    "# Run EM algorithm on 1000 images from the MNIST dataset. \n",
    "expectation_maximization = EM(n_components=k, max_iter=10, init_params='kmeans', covariance_type='diag', verbose=1, verbose_interval =1).fit(X)\n",
    "\n",
    "means = expectation_maximization.means_\n",
    "covs = expectation_maximization.covariances_\n",
    "      \n",
    "fig, ax = plt.subplots(1, k, figsize=(8, 1))\n",
    "\n",
    "for i in range(k):\n",
    "    ax[i].imshow(means[i].reshape(28, 28), cmap='gray')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "\n",
    "def sample(means, covs, num):\n",
    "    mean = means[num]\n",
    "    cov = covs[num]\n",
    "     \n",
    "    fig, ax = plt.subplots(1, 10, figsize=(8, 1))\n",
    "    \n",
    "    for i in range(10):\n",
    "        img = multivariate_normal.rvs(mean=mean, cov=cov) # draw random sample   \n",
    "        ax[i].imshow(img.reshape(28, 28), cmap='gray') # draw the random sample\n",
    "    plt.show()\n",
    "    \n",
    "sample(means, covs, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Report section 4: </b>\n",
    "- Explain why we can generate images of \"digits\" using the Expectation Maximization algorithm. \n",
    "- (optional) Why does the sampled images look <a hrf=\"https://boofcv.org/images/1/18/Example_lena_denoise_noisy.jpg\" target=\"new\">noisy</a>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "As part of the handin you must submit the following.\n",
    "\n",
    "### Code\n",
    "You must include a `.py` file including your implementations of the two algorithms (Lloyd and EM) and the two evaluation measures (Silhouette coefficient and F1 score). Alternatively, you could code everything in here and upload the final IPython Notebook. \n",
    "\n",
    "### Report\n",
    "Your report should be no more than 5 pages and clearly state who is in the group. It must cover:\n",
    "\n",
    "* Summary/Abstract: The status of the work, i.e., does it work, if not, then why.\n",
    "\n",
    "<b>Report section 1: </b>\n",
    "- Explain how you compared the results of Lloyd's algoithm and the Expectation Maximization algorithm. \n",
    "- Which algorithm performed best? \n",
    "\n",
    "<b>Report section 2.1: </b>\n",
    "- Include a tabel that holds the silhouette coefficient for the above experiment.\n",
    "- Which value of $k$ gave the best silhouette coefficient?\n",
    "\n",
    "<b>Report section 2.2: </b>\n",
    "- Include a tabel that holds the F1 score for the above experiment.\n",
    "- Which value of $k$ gave the best F1 score?\n",
    "\n",
    "<b>Report section 2.3: </b>Are there any differences between the two quality measures (F1 and Silhouette Coefficient)?\n",
    "\n",
    "HINT: External/internal measures supervised/unsupervised.\n",
    "\n",
    "<b>Report section 3: </b>\n",
    "- Include the original and the compressed image. You should state the original and compressed sizes and the compression ratio. \n",
    "- Briefly in 5 lines write what you did to compress you image. \n",
    "\n",
    "<b>Report section 4: </b>\n",
    "- Explain why we can generate images of digits using the Expectation Maximization algorithm. \n",
    "- (optional) Why does the sampled images look <a hrf=\"https://boofcv.org/images/1/18/Example_lena_denoise_noisy.jpg\" target=\"new\">noisy</a>?</div>\n",
    "\n",
    "<!--\n",
    "* A discussion of plots of at least two runs of your algorithm\n",
    "  implementations detailing what you can see. Make sure that you relate this\n",
    "  to the discussion in the lecture or textbook about the strengths and\n",
    "  weaknesses of the algorithms.\n",
    "* A discussion of plots of the evaluation measures F1 and silhouette\n",
    "  coefficient, detailing what you can learn from them. Include an explanation\n",
    "  of what the evaluation measures reflect. TODO: Compute for several k's?\n",
    "* Describe how you can use one of the clustering algorithms for image\n",
    "  compression, and demonstrate the results for at least one algorithm on both\n",
    "  images, discussing their quality and giving a reasoning for the differences. -->\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
